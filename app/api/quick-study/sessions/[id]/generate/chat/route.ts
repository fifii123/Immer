// app/api/quick-study/sessions/[id]/generate/chat/route.ts
export const runtime = 'nodejs'
import { NextRequest } from 'next/server'
import { OpenAI } from 'openai'
import { QuickStudyTextService } from '@/app/services/QuickStudyTextService'
import { Source, SessionData } from '@/app/types/QuickStudyTypes'

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

// Global in-memory store
declare global {
  var quickStudySessions: Map<string, SessionData> | undefined
}

const sessions = globalThis.quickStudySessions ?? new Map<string, SessionData>()
globalThis.quickStudySessions = sessions

export async function POST(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  try {
    const sessionId = params.id
    
    console.log(`üí¨ Enhanced Chat request for session: ${sessionId}`)
    
    // Parse request body
    const body = await request.json()
    const { sourceId, message, conversationHistory } = body
    
    if (!sourceId || !message) {
      return Response.json(
        { message: 'Missing sourceId or message' },
        { status: 400 }
      )
    }
    
    // Check if session exists
    const sessionData = sessions.get(sessionId)
    if (!sessionData) {
      console.log(`‚ùå Session not found: ${sessionId}`)
      return Response.json(
        { message: 'Session not found' },
        { status: 404 }
      )
    }
    
    // Find source
    const source = sessionData.sources.find(s => s.id === sourceId)
    if (!source) {
      console.log(`‚ùå Source not found: ${sourceId}`)
      return Response.json(
        { message: 'Source not found' },
        { status: 404 }
      )
    }
    
    if (source.status !== 'ready') {
      return Response.json(
        { message: 'Source is not ready for processing' },
        { status: 400 }
      )
    }
    
    console.log(`ü§ñ Processing enhanced chat message for source: ${source.name}`)
    
    // üöÄ NEW: Get processing statistics for monitoring
    const processingStats = QuickStudyTextService.getProcessingStats(source, 'chat')
    console.log(`üìä Processing Stats:`, {
      textSource: processingStats.textSource,
      originalLength: processingStats.originalLength,
      processedLength: processingStats.processedLength,
      optimizationQuality: processingStats.optimizationQuality,
      recommended: processingStats.recommendedForTask
    })
    
    // Create streaming response
    const stream = new ReadableStream({
      async start(controller) {
        try {
          await streamEnhancedChatResponse(
            source,
            message,
            conversationHistory || [],
            controller
          )
        } catch (error) {
          console.error('‚ùå Error in enhanced chat stream:', error)
          controller.error(error)
        }
      }
    })
    
    return new Response(stream, {
      headers: {
        'Content-Type': 'text/stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    })
    
  } catch (error) {
    console.error('‚ùå Error in enhanced chat endpoint:', error)
    
    return Response.json(
      { message: 'Enhanced chat request failed' },
      { status: 500 }
    )
  }
}

// Enhanced streaming chat response using QuickStudyTextService
async function streamEnhancedChatResponse(
  source: Source, 
  userMessage: string, 
  conversationHistory: any[],
  controller: ReadableStreamDefaultController
) {
  console.log(`ü§ñ Generating enhanced streaming chat response for: ${source.name}`)
  
  try {
    // üöÄ NEW: Create contextual system prompt using QuickStudyTextService
    const baseSystemPrompt = createBaseSystemPrompt(source)
    const enhancedSystemPrompt = QuickStudyTextService.createContextualPrompt(
      source, 
      'chat', 
      baseSystemPrompt
    )
    
    // üöÄ NEW: Get optimal text for processing
    const textResult = QuickStudyTextService.getProcessingText(source, 'chat')
    
    // Enhanced logging
    console.log(`üìù Using ${textResult.source} text for chat:`)
    console.log(`   - Length: ${textResult.text.length.toLocaleString()} characters`)
    if (textResult.stats?.compressionRatio) {
      console.log(`   - Compression: ${(textResult.stats.compressionRatio * 100).toFixed(1)}%`)
    }
    if (textResult.stats?.keyTopics?.length) {
      console.log(`   - Key topics: ${textResult.stats.keyTopics.slice(0, 5).join(', ')}${textResult.stats.keyTopics.length > 5 ? ` (+${textResult.stats.keyTopics.length - 5} more)` : ''}`)
    }
    
    // Prepare conversation context with enhanced material info
const messages = [
  { role: 'system', content: enhancedSystemPrompt },
  { 
    role: 'system', 
    content: `MATERIA≈Å ≈πR√ìD≈ÅOWY:
---
${textResult.text}
---` 
  },
  ...conversationHistory.map(msg => ({
    role: msg.role,
    content: msg.content
  })),
  { role: 'user', content: userMessage }
]
    
    console.log(`üîÑ Creating enhanced chat completion...`)
    
    // Create streaming completion
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: messages as any,
      temperature: 0.7,
      max_tokens: 1500,
      stream: true
    })
    
    let fullResponse = ''
    let chunkCount = 0
    
    // Stream the response with enhanced monitoring
    console.log(`üì§ Starting enhanced response stream...`)
    for await (const chunk of completion) {
      const content = chunk.choices[0]?.delta?.content || ''
      
      if (content) {
        fullResponse += content
        chunkCount++
        
        // Send chunk to client
        const data = JSON.stringify({ 
          type: 'chunk', 
          content: content,
          // Optional: include processing metadata for debugging
          meta: chunkCount === 1 ? {
            textSource: textResult.source,
            textLength: textResult.text.length,
            optimization: textResult.stats
          } : undefined
        })
        controller.enqueue(new TextEncoder().encode(`data: ${data}\n\n`))
      }
    }
    
    // Send completion signal with enhanced metadata
    const completeData = JSON.stringify({ 
      type: 'complete', 
      fullContent: fullResponse,
      processingInfo: {
        sourceType: textResult.source,
        processedLength: textResult.text.length,
        responseLength: fullResponse.length,
        chunkCount: chunkCount
      }
    })
    controller.enqueue(new TextEncoder().encode(`data: ${completeData}\n\n`))
    
    console.log(`‚úÖ Enhanced chat response completed successfully`)
    console.log(`   - Response length: ${fullResponse.length} characters`)
    console.log(`   - Chunks streamed: ${chunkCount}`)
    console.log(`   - Source: ${textResult.source}`)
    
    controller.close()
    
  } catch (error) {
    console.error('‚ùå Error streaming enhanced chat response:', error)
    
    // Send error to client
    const errorData = JSON.stringify({ 
      type: 'error', 
      message: 'Failed to generate enhanced response',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
    controller.enqueue(new TextEncoder().encode(`data: ${errorData}\n\n`))
    controller.close()
    
    throw error
  }
}

// Create base system prompt (without text processing context)
function createBaseSystemPrompt(source: Source): string {
  return `Jeste≈õ inteligentnym asystentem AI specjalizujƒÖcym siƒô w pomocy przy nauce. Odpowiadasz na pytania u≈ºytkownika na podstawie dostarczonego materia≈Çu ≈∫r√≥d≈Çowego.

WA≈ªNE ZASADY:
1. Odpowiadaj TYLKO na podstawie dostarczonego materia≈Çu ≈∫r√≥d≈Çowego
2. Je≈õli pytanie wykracza poza materia≈Ç, uprzejmie poinformuj o tym
3. U≈ºywaj jasnego, zrozumia≈Çego jƒôzyka
4. Dawaj konkretne przyk≈Çady z materia≈Çu gdy to mo≈ºliwe
5. Je≈õli nie jeste≈õ pewien, powiedz to wprost
6. Formatuj odpowiedzi w spos√≥b czytelny (u≈ºywaj akapit√≥w, list gdy potrzeba)
7. Przy cytowaniu lub odwo≈Çywaniu siƒô do konkretnych fragment√≥w, u≈ºywaj precyzyjnych odniesie≈Ñ

MATERIA≈Å ≈πR√ìD≈ÅOWY: "${source.name}" (${source.type})
Typ pliku: ${source.type}
${source.wordCount ? `Liczba s≈Ç√≥w: ${source.wordCount.toLocaleString()}` : ''}
${source.pages ? `Liczba stron: ${source.pages}` : ''}

INSTRUKCJE SPECJALNE:
- Je≈õli materia≈Ç zosta≈Ç zoptymalizowany, pamiƒôtaj ≈ºe zawiera najwa≈ºniejsze informacje w skoncentrowanej formie
- Przy odpowiedziach wykorzystuj wiedzƒô o kluczowych tematach je≈õli sƒÖ dostƒôpne
- Je≈õli u≈ºywasz fragmentu dokumentu, informuj o tym kontek≈õcie
- Zawsze bazuj na factach z materia≈Çu, nie dodawaj informacji z zewnƒÖtrz`
}

// üöÄ Enhanced logging helper for debugging
function logProcessingContext(source: Source, textResult: any) {
  console.log(`
=== üîç ENHANCED CHAT PROCESSING CONTEXT ===
üìÑ Source: ${source.name} (${source.type})
üìä Original length: ${source.extractedText?.length || 0} chars
üéØ Processing text: ${textResult.text.length} chars (${textResult.source})
${textResult.stats?.compressionRatio ? `üìà Compression: ${(textResult.stats.compressionRatio * 100).toFixed(1)}%` : ''}
${textResult.stats?.strategy ? `üé≤ Strategy: ${textResult.stats.strategy}` : ''}
${textResult.stats?.keyTopics ? `üè∑Ô∏è Topics: ${textResult.stats.keyTopics.slice(0, 3).join(', ')}` : ''}
${'='.repeat(50)}
`)
}